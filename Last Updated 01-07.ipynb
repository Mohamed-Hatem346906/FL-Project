{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93385246ed5248aeb759e9e84fad55de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ed94edb3b2d4e0aa299dc5c3dc19d47",
              "IPY_MODEL_2fbbbcef377d4179844db4e65dbd5129",
              "IPY_MODEL_617ae2ad41e24fe7affbd12c639f65aa"
            ],
            "layout": "IPY_MODEL_fc35f9d80cb14468848268cea38012cc"
          }
        },
        "6ed94edb3b2d4e0aa299dc5c3dc19d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c79dc7366e6442da9c919fd1e1afd533",
            "placeholder": "​",
            "style": "IPY_MODEL_24eee0d1da6c4a8ba52a5f7bfa4dd60c",
            "value": "model.safetensors: 100%"
          }
        },
        "2fbbbcef377d4179844db4e65dbd5129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aff877416e524e1cb8b6aed3af19c450",
            "max": 88216496,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f9e6536d8ec464fae5eaf83b6ba66b8",
            "value": 88216496
          }
        },
        "617ae2ad41e24fe7affbd12c639f65aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_614aa5a366e3427693a739b01e3c6d02",
            "placeholder": "​",
            "style": "IPY_MODEL_9d66af58767b4e819be39a0795c9a6f6",
            "value": " 88.2M/88.2M [00:00&lt;00:00, 160MB/s]"
          }
        },
        "fc35f9d80cb14468848268cea38012cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c79dc7366e6442da9c919fd1e1afd533": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24eee0d1da6c4a8ba52a5f7bfa4dd60c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aff877416e524e1cb8b6aed3af19c450": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f9e6536d8ec464fae5eaf83b6ba66b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "614aa5a366e3427693a739b01e3c6d02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d66af58767b4e819be39a0795c9a6f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Colab: Install dependencies\n",
        "!pip install timm\n",
        "# PyTorch core library for tensor computations and neural networks\n",
        "import torch\n",
        "\n",
        "# PyTorch vision utilities for computer vision tasks (datasets, transforms)\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Dictionary subclass that calls a factory function for missing keys\n",
        "from collections import defaultdict\n",
        "\n",
        "# Date and time manipulation\n",
        "import datetime\n",
        "\n",
        "# PyTorch data loading utilities\n",
        "from torch.utils.data import Subset, DataLoader, SubsetRandomSampler\n",
        "\n",
        "# Numerical computing library\n",
        "import numpy as np\n",
        "\n",
        "# Random number generation utilities\n",
        "import random\n",
        "\n",
        "# For deep copy operations\n",
        "import copy\n",
        "\n",
        "# PyTorch neural network modules\n",
        "import torch.nn as nn\n",
        "\n",
        "# PyTorch Image Models (timm) - collection of SOTA computer vision models\n",
        "import timm\n",
        "\n",
        "# Matplotlib's pyplot interface for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# High-level interface for statistical graphics (based on matplotlib)\n",
        "import seaborn as sns\n",
        "\n",
        "# Scikit-learn metrics for model evaluation\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# PyTorch optimization algorithms\n",
        "import torch.optim as optim\n",
        "\n",
        "# Operating system interfaces (file/directory operations)\n",
        "import os\n",
        "\n",
        "# Google Colab specific - for mounting Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Re-importing defaultdict (redundant, already imported above)\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "import time\n"
      ],
      "metadata": {
        "id": "8Zt1HMASI4uT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b2a582f-7714-4e81-f2b4-0a7e3847b926"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.33.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.6.15)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access files (force_remount ensures fresh mount)\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "class Config:\n",
        "    # Hardware & Paths\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    GOOGLE_DRIVE = '/content/drive/MyDrive/federated_model_editing'\n",
        "    CHECKPOINT_DIR = f\"{GOOGLE_DRIVE}/checkpoints\"\n",
        "    LOG_DIR = f\"{GOOGLE_DRIVE}/logs\"\n",
        "    FIGURE_DIR = f\"{GOOGLE_DRIVE}/figures\"\n",
        "    RANDOM_SEED = 42\n",
        "\n",
        "    # Dataset\n",
        "    DATASET = 'CIFAR100'\n",
        "    NUM_CLASSES = 100\n",
        "    VAL_SPLIT = 0.1\n",
        "    BATCH_SIZE = 64\n",
        "\n",
        "    # Model\n",
        "    MODEL_NAME = 'vit_small_patch16_224'\n",
        "\n",
        "    # Centralized Training\n",
        "    EPOCHS_CENTRALIZED = 10 #per experiment\n",
        "    LR = 0.03\n",
        "    MOMENTUM = 0.9\n",
        "    WEIGHT_DECAY = 5e-4\n",
        "\n",
        "    # Federated Learning\n",
        "    NUM_CLIENTS = 100\n",
        "    EPOCHS_FEDAVG = 50\n",
        "    LOCAL_EPOCHS = 5\n",
        "    CLIENT_FRACTION = 0.1\n",
        "\n",
        "    # Sparse/Masking\n",
        "    SPARSE_RATIO = 0.5\n",
        "    NC = 10  # Default non-iid classes per client\n",
        "\n",
        "    # Others\n",
        "    NUM_WORKERS = 2\n",
        "    PIN_MEMORY = True"
      ],
      "metadata": {
        "id": "MuGtuIWFJOrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3492a28c-3a48-4163-d90d-4a22453feba7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transforms():\n",
        "    \"\"\"\n",
        "    Standardized data transformations for CIFAR-100, resized for ViT input.\n",
        "    Returns:\n",
        "        tuple: (transform_train, transform_test)\n",
        "    \"\"\"\n",
        "    mean = (0.5071, 0.4867, 0.4408)\n",
        "    std = (0.2675, 0.2565, 0.2761)\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "    return transform_train, transform_test\n",
        "\n",
        "def split_dataset(dataset, val_split=0.1, seed=42):\n",
        "    \"\"\"\n",
        "    Split dataset into training and validation subsets.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    indices = np.arange(len(dataset))\n",
        "    np.random.shuffle(indices)\n",
        "    val_size = int(len(dataset) * val_split)\n",
        "    val_indices = indices[:val_size]\n",
        "    train_indices = indices[val_size:]\n",
        "    return Subset(dataset, train_indices), Subset(dataset, val_indices)\n",
        "\n",
        "def iid_shard(dataset, num_clients):\n",
        "    \"\"\"\n",
        "    IID partitioning: Equal-sized random shards per client.\n",
        "    \"\"\"\n",
        "    data_per_client = len(dataset) // num_clients\n",
        "    indices = np.arange(len(dataset))\n",
        "    np.random.shuffle(indices)\n",
        "    return [\n",
        "        indices[i * data_per_client : (i + 1) * data_per_client]\n",
        "        for i in range(num_clients)\n",
        "    ]\n",
        "\n",
        "def noniid_shard(dataset, num_clients, nc=1):\n",
        "    \"\"\"\n",
        "    Non-IID partitioning: Each client gets data from nc classes only.\n",
        "    \"\"\"\n",
        "    # Handle Subset or Dataset directly\n",
        "    if isinstance(dataset, Subset):\n",
        "        labels = np.array(dataset.dataset.targets)[dataset.indices]\n",
        "    else:\n",
        "        labels = np.array(dataset.targets)\n",
        "    num_classes = len(np.unique(labels))\n",
        "    idx_by_class = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    client_indices = [[] for _ in range(num_clients)]\n",
        "    classes_per_client = [\n",
        "        np.random.choice(num_classes, nc, replace=False)\n",
        "        for _ in range(num_clients)\n",
        "    ]\n",
        "    for client_id, client_classes in enumerate(classes_per_client):\n",
        "        for c in client_classes:\n",
        "            cnt = len(idx_by_class[c]) // num_clients\n",
        "            chosen = idx_by_class[c][:cnt]\n",
        "            idx_by_class[c] = idx_by_class[c][cnt:]\n",
        "            client_indices[client_id].extend(chosen.tolist())\n",
        "    return client_indices\n",
        "\n",
        "def get_dataloaders(config, iid=True, nc=10):\n",
        "    \"\"\"\n",
        "    Create federated dataloaders: per-client, validation, and test.\n",
        "    \"\"\"\n",
        "    transform_train, transform_test = get_transforms()\n",
        "    trainset = datasets.CIFAR100(\n",
        "        root='./data', train=True, download=True, transform=transform_train\n",
        "    )\n",
        "    testset = datasets.CIFAR100(\n",
        "        root='./data', train=False, download=True, transform=transform_test\n",
        "    )\n",
        "    trainset, valset = split_dataset(trainset, val_split=config.VAL_SPLIT)\n",
        "    # Shard for each client (by indices)\n",
        "    shards = (\n",
        "        iid_shard(trainset, config.NUM_CLIENTS)\n",
        "        if iid else noniid_shard(trainset, config.NUM_CLIENTS, nc)\n",
        "    )\n",
        "    clients_loaders = [\n",
        "        DataLoader(\n",
        "            trainset,\n",
        "            batch_size=config.BATCH_SIZE,\n",
        "            sampler=SubsetRandomSampler(idxs),\n",
        "            num_workers=config.NUM_WORKERS,\n",
        "            pin_memory=config.PIN_MEMORY\n",
        "        )\n",
        "        for idxs in shards\n",
        "    ]\n",
        "    val_loader = DataLoader(\n",
        "        valset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=config.NUM_WORKERS,\n",
        "        pin_memory=config.PIN_MEMORY\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        testset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=config.NUM_WORKERS,\n",
        "        pin_memory=config.PIN_MEMORY\n",
        "    )\n",
        "    return clients_loaders, val_loader, test_loader"
      ],
      "metadata": {
        "id": "tBB_parsI9gk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTSmallDINO(nn.Module):\n",
        "    \"\"\"Vision Transformer (ViT) model with DINO-pretrained weights.\n",
        "\n",
        "    This class implements a small Vision Transformer (ViT-S/16) using pretrained weights\n",
        "    from DINO (self-supervised learning). The model consists of:\n",
        "    - A DINO-pretrained ViT backbone\n",
        "    - A custom linear classification head\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Number of output classes for classification (default: 100)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=100):\n",
        "        \"\"\"Initialize the ViT model with DINO pretrained weights.\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): Number of classes for the classification head\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pretrained ViT-S/16 model from timm with DINO weights\n",
        "        # Patch size: 16x16, Image size: 224x224\n",
        "        self.backbone = timm.create_model('vit_small_patch16_224', pretrained=True)\n",
        "\n",
        "        # Replace the original head with a new linear layer for our task\n",
        "        # in_features: Dimension of the backbone's output features\n",
        "        # out_features: Number of target classes\n",
        "        self.backbone.head = nn.Linear(self.backbone.head.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, 3, 224, 224)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output logits of shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        # Pass input through the backbone (ViT + classification head)\n",
        "        return self.backbone(x)\n",
        "\n",
        "\n",
        "def get_model(num_classes=100, device='cuda'):\n",
        "    \"\"\"Convenience function to create and configure the ViT-DINO model.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Number of output classes (default: 100)\n",
        "        device (str): Device to load the model onto ('cuda' or 'cpu') (default: 'cuda')\n",
        "\n",
        "    Returns:\n",
        "        ViTSmallDINO: Model instance moved to the specified device\n",
        "    \"\"\"\n",
        "    # Initialize the ViT-DINO model with specified number of classes\n",
        "    model = ViTSmallDINO(num_classes=num_classes)\n",
        "\n",
        "    # Move model to the specified device (GPU if available)\n",
        "    return model.to(device)"
      ],
      "metadata": {
        "id": "uF2Vuj02JTps"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SparseSGDM(optim.SGD):\n",
        "    \"\"\"Sparse Stochastic Gradient Descent with Momentum (SparseSGDM) optimizer.\n",
        "\n",
        "    This optimizer extends PyTorch's SGD with support for gradient masking, enabling\n",
        "    sparse parameter updates during training. Only parameters corresponding to non-zero\n",
        "    values in the mask will be updated.\n",
        "\n",
        "    Key Features:\n",
        "    - Inherits all standard SGD functionality (momentum, weight decay)\n",
        "    - Applies binary mask to gradients before parameter updates\n",
        "    - Maintains original SGD performance for unmasked parameters\n",
        "    - Device-aware mask application (automatically moves mask to parameter device)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=0.01, momentum=0, weight_decay=0, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            params (iterable): Iterable of parameters to optimize or dicts defining parameter groups\n",
        "            lr (float): Learning rate (default: 0.01)\n",
        "            momentum (float): Momentum factor (default: 0)\n",
        "            weight_decay (float): Weight decay (L2 penalty) (default: 0)\n",
        "            mask (list[Tensor], optional): List of binary masks (0/1) corresponding to\n",
        "                parameter tensors. Must match parameter structure. None means no masking.\n",
        "        \"\"\"\n",
        "        super().__init__(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "        self.mask = mask\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step with gradient masking.\n",
        "\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model and returns the loss.\n",
        "\n",
        "        Returns:\n",
        "            float: The computed loss if closure is provided, otherwise None\n",
        "\n",
        "        Note:\n",
        "            - Mask is applied BEFORE the standard SGD step calculation\n",
        "            - Only parameters with non-None gradients are masked\n",
        "            - Mask is automatically moved to the parameter's device\n",
        "            - If mask is None, behaves exactly like standard SGD\n",
        "        \"\"\"\n",
        "        # Apply gradient mask before the parameter update!\n",
        "        if self.mask:\n",
        "            for group in self.param_groups:\n",
        "                for i, param in enumerate(group['params']):\n",
        "                    if param.grad is not None and i < len(self.mask):\n",
        "                        param.grad *= self.mask[i].to(param.grad.device)\n",
        "        loss = super().step(closure)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "w77QvlYfJqvi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_weights(weights_list):\n",
        "    \"\"\"Compute element-wise average of multiple model state dictionaries.\n",
        "\n",
        "    Args:\n",
        "        weights_list (list[dict]): List of model state dictionaries to average\n",
        "\n",
        "    Returns:\n",
        "        dict: Averaged state dictionary with same structure as inputs\n",
        "    \"\"\"\n",
        "    avg_weights = copy.deepcopy(weights_list[0])\n",
        "    for key in avg_weights.keys():\n",
        "        for i in range(1, len(weights_list)):\n",
        "            avg_weights[key] += weights_list[i][key]\n",
        "        avg_weights[key] = torch.div(avg_weights[key], len(weights_list))\n",
        "    return avg_weights\n",
        "\n",
        "def fedavg_round(model, clients_loaders, config, train_fn, mask=None, device='cuda'):\n",
        "    \"\"\"Execute one communication round of Federated Averaging (FedAvg).\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Global model to update\n",
        "        clients_loaders (list[DataLoader]): List of client data loaders\n",
        "        config (object): Configuration object (NUM_CLIENTS, CLIENT_FRACTION, LR, MOMENTUM, WEIGHT_DECAY, LOCAL_EPOCHS)\n",
        "        train_fn (callable): Function to perform client training (model, loader, optimizer, criterion, device, mask)\n",
        "        mask (list[Tensor], optional): Gradient mask for sparse training\n",
        "        device (str): Device to use ('cuda' or 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: Updated global model after aggregation\n",
        "    \"\"\"\n",
        "    num_selected = max(1, int(config.NUM_CLIENTS * config.CLIENT_FRACTION))\n",
        "    selected_clients = random.sample(range(len(clients_loaders)), num_selected)\n",
        "    client_weights = []\n",
        "    for client_idx in selected_clients:\n",
        "        local_model = copy.deepcopy(model)\n",
        "        optimizer = torch.optim.SGD(\n",
        "            local_model.parameters(),\n",
        "            lr=config.LR,\n",
        "            momentum=config.MOMENTUM,\n",
        "            weight_decay=config.WEIGHT_DECAY\n",
        "        )\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        for _ in range(config.LOCAL_EPOCHS):\n",
        "            train_fn(local_model, clients_loaders[client_idx], optimizer, criterion, device, mask)\n",
        "        client_weights.append(copy.deepcopy(local_model.state_dict()))\n",
        "        del local_model\n",
        "        torch.cuda.empty_cache()\n",
        "    model.load_state_dict(average_weights(client_weights))\n",
        "    return model"
      ],
      "metadata": {
        "id": "xUiPRsoXJzwN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, criterion, device, mask=None):\n",
        "    \"\"\"Train model for one epoch with optional gradient masking.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Model to train\n",
        "        loader (DataLoader): Training data loader\n",
        "        optimizer (Optimizer): Optimizer for parameter updates\n",
        "        criterion (nn.Module): Loss function\n",
        "        device (torch.device): Device to use ('cuda' or 'cpu')\n",
        "        mask (list[Tensor], optional): Gradient mask for sparse training\n",
        "\n",
        "    Returns:\n",
        "        tuple: (epoch_loss, epoch_accuracy) averaged over all batches\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        # If using a SparseSGDM or similar optimizer, assign mask before step\n",
        "        if mask is not None and hasattr(optimizer, 'mask'):\n",
        "            optimizer.mask = mask\n",
        "        optimizer.step()\n",
        "        batch_size = targets.size(0)\n",
        "        running_loss += loss.item() * batch_size\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        total += batch_size\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_accuracy = correct / total\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    \"\"\"Evaluate model performance on validation/test set.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Model to evaluate\n",
        "        loader (DataLoader): Evaluation data loader\n",
        "        criterion (nn.Module): Loss function\n",
        "        device (torch.device): Device to use ('cuda' or 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        tuple: (epoch_loss, epoch_accuracy) averaged over all batches\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            batch_size = targets.size(0)\n",
        "            running_loss += loss.item() * batch_size\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += batch_size\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_accuracy = correct / total\n",
        "    return epoch_loss, epoch_accuracy"
      ],
      "metadata": {
        "id": "Wnd4EmP4KQQk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fisher_sensitivity(model, dataloader, device, num_samples=128):\n",
        "    \"\"\"Compute diagonal Fisher Information Matrix (FIM) for model parameters.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Model to analyze\n",
        "        dataloader (DataLoader): Data for computing empirical Fisher\n",
        "        device (torch.device): Device to perform computation on\n",
        "        num_samples (int): Maximum number of samples to use (default: 128)\n",
        "\n",
        "    Returns:\n",
        "        list[Tensor]: Per-parameter Fisher sensitivity scores (FIM diagonal)\n",
        "    \"\"\"\n",
        "    model.eval()  # Disable dropout/batchnorm\n",
        "    grads = []\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    total_samples = 0\n",
        "\n",
        "    for _, (inputs, targets) in enumerate(dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        model.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        grads.append([p.grad.detach().clone() for p in model.parameters() if p.requires_grad])\n",
        "        total_samples += inputs.size(0)\n",
        "        if total_samples >= num_samples:\n",
        "            break\n",
        "\n",
        "    # Compute mean squared gradients (empirical Fisher diagonal)\n",
        "    fisher_diagonal = [\n",
        "        (torch.stack(params) ** 2).mean(dim=0)\n",
        "        for params in zip(*grads)\n",
        "    ]\n",
        "    return fisher_diagonal\n",
        "\n",
        "def calibrate_mask(model, dataloader, device, method=\"least-sensitive\", sparsity=0.2, seed=42):\n",
        "    \"\"\"Generate binary mask for sparse training using specified strategy.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Model to analyze\n",
        "        dataloader (DataLoader): Data for sensitivity analysis\n",
        "        device (torch.device): Computation device\n",
        "        method (str): Masking strategy\n",
        "        sparsity (float): Fraction of weights to keep\n",
        "        seed (int): Random seed\n",
        "\n",
        "    Returns:\n",
        "        list[Tensor]: Binary masks (0=prune, 1=keep) matching model parameters\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    fisher = fisher_sensitivity(model, dataloader, device)\n",
        "    mask = []\n",
        "    for param_fisher in fisher:\n",
        "        scores = param_fisher.view(-1)\n",
        "        k = int(scores.numel() * sparsity)\n",
        "        if method == \"least-sensitive\":\n",
        "            idx = torch.topk(scores, k, largest=False).indices\n",
        "        elif method == \"most-sensitive\":\n",
        "            idx = torch.topk(scores, k, largest=True).indices\n",
        "        elif method == \"low-magnitude\":\n",
        "            idx = torch.topk(scores.abs(), k, largest=False).indices\n",
        "        elif method == \"high-magnitude\":\n",
        "            idx = torch.topk(scores.abs(), k, largest=True).indices\n",
        "        elif method == \"random\":\n",
        "            idx = torch.randperm(scores.numel())[:k]\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown masking method: {method}\")\n",
        "        param_mask = torch.zeros_like(scores)\n",
        "        param_mask[idx] = 1\n",
        "        mask.append(param_mask.view(param_fisher.shape))\n",
        "    return mask\n",
        "\n",
        "def hybrid_mask(model, dataloader, device, sparsity=0.5, alpha=0.5):\n",
        "    \"\"\"Create hybrid mask combining Fisher sensitivity and weight magnitude.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Model to analyze\n",
        "        dataloader (DataLoader): Data for Fisher computation\n",
        "        device (torch.device): Computation device\n",
        "        sparsity (float): Fraction of weights to keep\n",
        "        alpha (float): Mixing coefficient (0=magnitude only, 1=Fisher only)\n",
        "\n",
        "    Returns:\n",
        "        list[Tensor]: Binary masks matching model parameters\n",
        "    \"\"\"\n",
        "    fisher = fisher_sensitivity(model, dataloader, device)\n",
        "    magnitudes = [p.abs().detach() for p in model.parameters() if p.requires_grad]\n",
        "    hybrid_mask = []\n",
        "    for f_score, m_score in zip(fisher, magnitudes):\n",
        "        combined_score = alpha * f_score + (1 - alpha) * m_score\n",
        "        k = int(combined_score.numel() * sparsity)\n",
        "        idx = torch.topk(combined_score.view(-1), k, largest=False).indices\n",
        "        mask = torch.zeros_like(combined_score.view(-1))\n",
        "        mask[idx] = 1\n",
        "        hybrid_mask.append(mask.view(f_score.shape))\n",
        "    return hybrid_mask"
      ],
      "metadata": {
        "id": "tnvrgGcRJ6Hf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "def plot_curves(train_loss, val_loss, train_acc, val_acc, save_path=None, title=None):\n",
        "    \"\"\"Plot training/validation loss and accuracy curves.\n",
        "\n",
        "    Args:\n",
        "        train_loss (list): Training loss values per epoch\n",
        "        val_loss (list): Validation loss values per epoch\n",
        "        train_acc (list): Training accuracy values per epoch\n",
        "        val_acc (list): Validation accuracy values per epoch\n",
        "        save_path (str, optional): Path to save figure. If None, shows plot.\n",
        "        title (str, optional): Overall title for the figure.\n",
        "\n",
        "    Displays:\n",
        "        Matplotlib figure with two subplots:\n",
        "        - Left: Loss curves (train and validation)\n",
        "        - Right: Accuracy curves (train and validation)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))  # Set figure size\n",
        "\n",
        "    # Plot loss curves\n",
        "    plt.subplot(1, 2, 1)  # 1 row, 2 cols, first plot\n",
        "    plt.plot(train_loss, label='Train Loss', marker='o', markersize=3)\n",
        "    plt.plot(val_loss, label='Val Loss', marker='o', markersize=3)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)  # Semi-transparent grid\n",
        "    plt.title(\"Loss Curves\")\n",
        "\n",
        "    # Plot accuracy curves\n",
        "    plt.subplot(1, 2, 2)  # 1 row, 2 cols, second plot\n",
        "    plt.plot(train_acc, label='Train Acc', marker='o', markersize=3)\n",
        "    plt.plot(val_acc, label='Val Acc', marker='o', markersize=3)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.title(\"Accuracy Curves\")\n",
        "\n",
        "    # Add overall title if provided\n",
        "    if title is not None:\n",
        "        plt.suptitle(title)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust for suptitle\n",
        "    else:\n",
        "        plt.tight_layout()  # Prevent label overlap\n",
        "\n",
        "    # Save or display\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')  # High quality save\n",
        "    plt.show()\n",
        "\n",
        "def plot_conf_matrix(model, loader, class_names, device, normalize='true', save_path=None):\n",
        "    \"\"\"Plot confusion matrix for model predictions.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Trained model to evaluate\n",
        "        loader (DataLoader): Data loader for evaluation set\n",
        "        class_names (list): Names of classes for axis labels\n",
        "        device (torch.device): Device to run model on\n",
        "        normalize (str): Normalization mode ('true', 'pred', 'all' or None)\n",
        "        save_path (str, optional): Path to save figure. If None, shows plot.\n",
        "\n",
        "    Displays:\n",
        "        Matplotlib figure showing normalized confusion matrix with:\n",
        "        - Class labels on axes\n",
        "        - Color-coded prediction percentages\n",
        "        - Vertical x-axis labels for readability\n",
        "    \"\"\"\n",
        "    # Collect all predictions and targets\n",
        "    y_true, y_pred = [], []\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for inputs, targets in loader:\n",
        "            # Move data to device and get predictions\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predictions = outputs.max(1)  # Get class indices\n",
        "\n",
        "            # Store results\n",
        "            y_true.extend(targets.cpu().numpy())\n",
        "            y_pred.extend(predictions.cpu().numpy())\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred, normalize=normalize)\n",
        "\n",
        "    # Create display object\n",
        "    disp = ConfusionMatrixDisplay(\n",
        "        confusion_matrix=cm,\n",
        "        display_labels=class_names\n",
        "    )\n",
        "\n",
        "    # Create figure and plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    disp.plot(\n",
        "        ax=ax,\n",
        "        xticks_rotation='vertical',  # Rotate x-axis labels\n",
        "        cmap='Blues',  # Color scheme\n",
        "        values_format='.2f' if normalize else 'd'  # Format based on normalization\n",
        "    )\n",
        "    plt.title('Confusion Matrix')\n",
        "\n",
        "    # Save or display\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')  # High quality save\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FNNfxN33KAVL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import itertools\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def run_centralized_training(\n",
        "    patience=7, n_epochs=None, lr=None, batch_size=None, weight_decay=None, seed=None, checkpoint_path=None, visualize=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Centralized training with progress and reproducibility.\n",
        "    \"\"\"\n",
        "    config = Config()\n",
        "    if lr: config.LR = lr\n",
        "    if batch_size: config.BATCH_SIZE = batch_size\n",
        "    if weight_decay: config.WEIGHT_DECAY = weight_decay\n",
        "    if seed: config.RANDOM_SEED = seed\n",
        "    if n_epochs: config.EPOCHS_CENTRALIZED = n_epochs\n",
        "    os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
        "    if checkpoint_path is None:\n",
        "        checkpoint_path = os.path.join(\n",
        "            config.CHECKPOINT_DIR,\n",
        "            f\"centralized_lr{config.LR}_bs{config.BATCH_SIZE}_wd{config.WEIGHT_DECAY}_seed{config.RANDOM_SEED}.pth\"\n",
        "        )\n",
        "\n",
        "    torch.manual_seed(config.RANDOM_SEED)\n",
        "    np.random.seed(config.RANDOM_SEED)\n",
        "    random.seed(config.RANDOM_SEED)\n",
        "\n",
        "    transform_train, transform_test = get_transforms()\n",
        "    full_trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    trainset, valset = split_dataset(full_trainset, val_split=config.VAL_SPLIT, seed=config.RANDOM_SEED)\n",
        "    train_loader = DataLoader(trainset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=config.PIN_MEMORY)\n",
        "    val_loader = DataLoader(valset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS, pin_memory=config.PIN_MEMORY)\n",
        "    test_loader = DataLoader(testset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS, pin_memory=config.PIN_MEMORY)\n",
        "    class_names = [str(i) for i in range(config.NUM_CLASSES)]\n",
        "\n",
        "    model = get_model(config.NUM_CLASSES, config.DEVICE)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=config.LR, momentum=config.MOMENTUM, weight_decay=config.WEIGHT_DECAY)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.EPOCHS_CENTRALIZED)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_acc = 0\n",
        "    best_model_state = None\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "    start_epoch = 0\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=config.DEVICE)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        train_losses = checkpoint['train_losses']\n",
        "        val_losses = checkpoint['val_losses']\n",
        "        train_accs = checkpoint['train_accs']\n",
        "        val_accs = checkpoint['val_accs']\n",
        "        best_val_acc = checkpoint['best_val_acc']\n",
        "        best_model_state = checkpoint['best_model_state']\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "    total_epochs = config.EPOCHS_CENTRALIZED\n",
        "    train_start_time = time.time()\n",
        "    for epoch in range(start_epoch, total_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        tr_loss, tr_acc = train(model, train_loader, optimizer, criterion, config.DEVICE)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, config.DEVICE)\n",
        "        train_losses.append(tr_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(tr_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "            epochs_no_improve = 0\n",
        "            print(f\"New best validation accuracy: {best_val_acc:.3f}\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        scheduler.step()\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        elapsed = time.time() - train_start_time\n",
        "        percent = 100 * (epoch - start_epoch + 1) / (total_epochs - start_epoch)\n",
        "        avg_epoch_time = elapsed / (epoch - start_epoch + 1)\n",
        "        eta = avg_epoch_time * ((total_epochs - start_epoch) - (epoch - start_epoch + 1))\n",
        "        print(f\"Epoch {epoch+1}/{total_epochs} \"\n",
        "              f\"({percent:.1f}%) - \"\n",
        "              f\"Train Acc: {tr_acc:.3f}, Val Acc: {val_acc:.3f} \"\n",
        "              f\"| Epoch Time: {epoch_time:.1f}s, \"\n",
        "              f\"Elapsed: {elapsed/60:.1f}m, ETA: {eta/60:.1f}m\")\n",
        "\n",
        "        # Fix: ensure parent directory exists before saving\n",
        "        os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_model_state': best_model_state,\n",
        "            'best_val_acc': best_val_acc,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'train_accs': train_accs,\n",
        "            'val_accs': val_accs,\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch} - no improvement in {patience} epochs.\")\n",
        "            break\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, config.DEVICE)\n",
        "\n",
        "    print(\"\\n=== Training Complete ===\")\n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.3f}\")\n",
        "    print(f\"Final Test Accuracy: {test_acc:.3f}\")\n",
        "\n",
        "    if visualize:\n",
        "        plot_curves(train_losses, val_losses, train_accs, val_accs)\n",
        "        plot_conf_matrix(model, test_loader, class_names, config.DEVICE)\n",
        "\n",
        "    return {\n",
        "        \"val_acc\": best_val_acc,\n",
        "        \"test_acc\": test_acc,\n",
        "        \"train_losses\": train_losses,\n",
        "        \"val_losses\": val_losses,\n",
        "        \"train_accs\": train_accs,\n",
        "        \"val_accs\": val_accs,\n",
        "        \"best_model_state\": best_model_state,\n",
        "        \"final_model_state\": model.state_dict()\n",
        "    }\n",
        "\n",
        "def hyperparam_grid_search(run_func, param_grid, n_seeds=2, experiment_name=\"\", result_key=\"val_acc\", **kwargs):\n",
        "    \"\"\"\n",
        "    Grid search for given training function with progress and timing.\n",
        "    \"\"\"\n",
        "    keys, values = zip(*param_grid.items())\n",
        "    configs = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "    results = []\n",
        "    total_configs = len(configs)\n",
        "    grid_start_time = time.time()\n",
        "\n",
        "    # Fix: ensure checkpoint directory exists before starting grid search\n",
        "    os.makedirs(\"./checkpoints\", exist_ok=True)\n",
        "\n",
        "    for idx, config in enumerate(configs, 1):\n",
        "        config_start_time = time.time()\n",
        "        scores = []\n",
        "        print(f\"\\n=== {experiment_name} config {idx}/{total_configs}: {config} ===\")\n",
        "        for seed in range(n_seeds):\n",
        "            print(f\"  [Seed {seed}]\")\n",
        "            run_start_time = time.time()\n",
        "            # Unique checkpoint path per config/seed\n",
        "            checkpoint_path = os.path.join(\n",
        "                \"./checkpoints\",\n",
        "                f\"{experiment_name.lower()}_lr{config['lr']}_bs{config['batch_size']}_wd{config['weight_decay']}_seed{seed}.pth\"\n",
        "            )\n",
        "            output = run_func(seed=seed, checkpoint_path=checkpoint_path, **config, **kwargs)\n",
        "            run_duration = time.time() - run_start_time\n",
        "            key_val = output.get(result_key, None)\n",
        "            if key_val is not None:\n",
        "                scores.append(key_val)\n",
        "            print(f\"    Finished seed {seed} in {run_duration:.1f}s | {result_key}: {key_val}\")\n",
        "        avg_score = float(np.mean(scores)) if scores else float('nan')\n",
        "        config_duration = time.time() - config_start_time\n",
        "        result = dict(config)\n",
        "        result[f\"avg_{result_key}\"] = avg_score\n",
        "        results.append(result)\n",
        "        print(f\"Config {idx}/{total_configs} finished in {config_duration:.1f}s | Avg {result_key}: {avg_score:.4f}\")\n",
        "    results = sorted(results, key=lambda d: d[f\"avg_{result_key}\"], reverse=True)\n",
        "    total_duration = time.time() - grid_start_time\n",
        "    print(f\"\\nBest configuration for {experiment_name}: {results[0]}\")\n",
        "    print(f\"\\nGrid search completed in {total_duration/60:.1f} minutes ({total_duration:.1f} seconds)\")\n",
        "    return results\n",
        "\n",
        "# Example grid search usage:\n",
        "param_grid_centralized = {\n",
        "    \"lr\": [0.01, 0.03, 0.05],\n",
        "    \"batch_size\": [64, 128],\n",
        "    \"weight_decay\": [5e-4]\n",
        "}\n",
        "\n",
        "results_central = hyperparam_grid_search(\n",
        "    run_centralized_training,\n",
        "    param_grid_centralized,\n",
        "    n_seeds=2,\n",
        "    experiment_name=\"Centralized\",\n",
        "    result_key=\"val_acc\",\n",
        "    n_epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312,
          "referenced_widgets": [
            "93385246ed5248aeb759e9e84fad55de",
            "6ed94edb3b2d4e0aa299dc5c3dc19d47",
            "2fbbbcef377d4179844db4e65dbd5129",
            "617ae2ad41e24fe7affbd12c639f65aa",
            "fc35f9d80cb14468848268cea38012cc",
            "c79dc7366e6442da9c919fd1e1afd533",
            "24eee0d1da6c4a8ba52a5f7bfa4dd60c",
            "aff877416e524e1cb8b6aed3af19c450",
            "1f9e6536d8ec464fae5eaf83b6ba66b8",
            "614aa5a366e3427693a739b01e3c6d02",
            "9d66af58767b4e819be39a0795c9a6f6"
          ]
        },
        "id": "63X6kRE06WEl",
        "outputId": "8369a904-feb4-4291-906b-20878b12e68a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Centralized config 1/6: {'lr': 0.01, 'batch_size': 64, 'weight_decay': 0.0005} ===\n",
            "  [Seed 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:04<00:00, 36.8MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/88.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93385246ed5248aeb759e9e84fad55de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best validation accuracy: 0.760\n",
            "Epoch 1/10 (10.0%) - Train Acc: 0.689, Val Acc: 0.760 | Epoch Time: 470.6s, Elapsed: 7.8m, ETA: 70.6m\n",
            "New best validation accuracy: 0.805\n",
            "Epoch 2/10 (20.0%) - Train Acc: 0.817, Val Acc: 0.805 | Epoch Time: 481.0s, Elapsed: 15.9m, ETA: 63.7m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_fedavg_iid_experiment():\n",
        "    config = Config()\n",
        "    os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
        "    checkpoint_path = os.path.join(config.CHECKPOINT_DIR, \"fedavg_iid.pth\")\n",
        "    torch.manual_seed(config.RANDOM_SEED)\n",
        "    np.random.seed(config.RANDOM_SEED)\n",
        "    random.seed(config.RANDOM_SEED)\n",
        "\n",
        "    clients_loaders, val_loader, test_loader = get_dataloaders(config, iid=True, nc=config.NUM_CLASSES)\n",
        "    model = get_model(config.NUM_CLASSES, config.DEVICE)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=config.LR, momentum=config.MOMENTUM, weight_decay=config.WEIGHT_DECAY)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    best_val_acc = 0\n",
        "    best_model_state = None\n",
        "    val_losses, val_accs = [], []\n",
        "    start_round = 0\n",
        "    NUM_ROUNDS = config.EPOCHS_FEDAVG\n",
        "    LOCAL_EPOCHS = config.LOCAL_EPOCHS\n",
        "    CLIENT_FRACTION = config.CLIENT_FRACTION\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=config.DEVICE)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        val_losses = checkpoint['val_losses']\n",
        "        val_accs = checkpoint['val_accs']\n",
        "        best_val_acc = checkpoint['best_val_acc']\n",
        "        best_model_state = checkpoint['best_model_state']\n",
        "        start_round = checkpoint['round'] + 1\n",
        "\n",
        "    for round_num in range(start_round, NUM_ROUNDS):\n",
        "        selected_clients = np.random.choice(\n",
        "            len(clients_loaders),\n",
        "            max(1, int(CLIENT_FRACTION * len(clients_loaders))),\n",
        "            replace=False\n",
        "        )\n",
        "        client_models = []\n",
        "        for client_idx in selected_clients:\n",
        "            client_model = copy.deepcopy(model)\n",
        "            client_optimizer = torch.optim.SGD(\n",
        "                client_model.parameters(),\n",
        "                lr=config.LR,\n",
        "                momentum=config.MOMENTUM,\n",
        "                weight_decay=config.WEIGHT_DECAY\n",
        "            )\n",
        "            for _ in range(LOCAL_EPOCHS):\n",
        "                train(client_model, clients_loaders[client_idx], client_optimizer, criterion, config.DEVICE)\n",
        "            client_models.append(client_model)\n",
        "        # Aggregate parameters (FedAvg)\n",
        "        with torch.no_grad():\n",
        "            for param in model.parameters():\n",
        "                param.data.zero_()\n",
        "            for client_model in client_models:\n",
        "                for server_param, client_param in zip(model.parameters(), client_model.parameters()):\n",
        "                    server_param.data += client_param.data / len(selected_clients)\n",
        "        # Periodic validation\n",
        "        if (round_num + 1) % 5 == 0 or round_num == NUM_ROUNDS - 1:\n",
        "            val_loss, val_acc = evaluate(model, val_loader, criterion, config.DEVICE)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accs.append(val_acc)\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_model_state = copy.deepcopy(model.state_dict())\n",
        "            print(f\"Round {round_num + 1}/{NUM_ROUNDS}: Val Acc {val_acc:.4f}\")\n",
        "            torch.save({\n",
        "                'round': round_num,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_model_state': best_model_state,\n",
        "                'best_val_acc': best_val_acc,\n",
        "                'val_losses': val_losses,\n",
        "                'val_accs': val_accs,\n",
        "            }, checkpoint_path)\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, config.DEVICE)\n",
        "    print(f\"Best Val Acc: {best_val_acc:.3f}\")\n",
        "    print(f\"Test Acc: {test_acc:.3f}\")\n",
        "    plot_curves([], val_losses, [], val_accs)"
      ],
      "metadata": {
        "id": "C5WirdsJKfql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_fedavg_iid_experiment()"
      ],
      "metadata": {
        "id": "b0SPiNivDQst",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "bb966b1b-20e1-402b-ef7a-3e64d7b073f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-3572008225.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_fedavg_iid_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-13-2403218908.py\u001b[0m in \u001b[0;36mrun_fedavg_iid_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m             )\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOCAL_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclients_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclient_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mclient_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Aggregate parameters (FedAvg)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-7-2390643012.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, device, mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_fedavg_noniid_experiments():\n",
        "    \"\"\"\n",
        "    Runs FedAvg experiments over various non-IID settings and local epochs,\n",
        "    collects results, and presents them in a table and heatmap.\n",
        "    Returns:\n",
        "        df: DataFrame summarizing results.\n",
        "    \"\"\"\n",
        "    config = Config()\n",
        "    os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
        "    checkpoint_dir = os.path.join(config.CHECKPOINT_DIR, \"noniid\")\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    nc_values = [1, 5, 10, 50]\n",
        "    local_epochs_values = [4, 8, 16]\n",
        "    results = {}\n",
        "\n",
        "    for nc in nc_values:\n",
        "        for local_epochs in local_epochs_values:\n",
        "            print(f\"\\n=== FedAvg non-iid: nc={nc}, local_epochs={local_epochs} ===\")\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, f\"fedavg_nc{nc}_le{local_epochs}.pth\")\n",
        "            torch.manual_seed(config.RANDOM_SEED)\n",
        "            np.random.seed(config.RANDOM_SEED)\n",
        "            random.seed(config.RANDOM_SEED)\n",
        "\n",
        "            clients_loaders, val_loader, test_loader = get_dataloaders(config, iid=False, nc=nc)\n",
        "            model = get_model(config.NUM_CLASSES, config.DEVICE)\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=config.LR, momentum=config.MOMENTUM, weight_decay=config.WEIGHT_DECAY)\n",
        "            criterion = torch.nn.CrossEntropyLoss()\n",
        "            best_val_acc = 0\n",
        "            best_model_state = None\n",
        "            val_losses, val_accs = [], []\n",
        "            start_round = 0\n",
        "            NUM_ROUNDS = config.EPOCHS_FEDAVG\n",
        "\n",
        "            # Resume if checkpoint exists\n",
        "            if os.path.exists(checkpoint_path):\n",
        "                checkpoint = torch.load(checkpoint_path, map_location=config.DEVICE)\n",
        "                model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "                val_losses = checkpoint['val_losses']\n",
        "                val_accs = checkpoint['val_accs']\n",
        "                best_val_acc = checkpoint['best_val_acc']\n",
        "                best_model_state = checkpoint['best_model_state']\n",
        "                start_round = checkpoint['round'] + 1\n",
        "\n",
        "            for round_num in range(start_round, NUM_ROUNDS):\n",
        "                selected_clients = np.random.choice(\n",
        "                    len(clients_loaders),\n",
        "                    max(1, int(config.CLIENT_FRACTION * len(clients_loaders))),\n",
        "                    replace=False\n",
        "                )\n",
        "                client_models = []\n",
        "                for client_idx in selected_clients:\n",
        "                    client_model = copy.deepcopy(model)\n",
        "                    client_optimizer = torch.optim.SGD(\n",
        "                        client_model.parameters(),\n",
        "                        lr=config.LR,\n",
        "                        momentum=config.MOMENTUM,\n",
        "                        weight_decay=config.WEIGHT_DECAY\n",
        "                    )\n",
        "                    for _ in range(local_epochs):\n",
        "                        train(client_model, clients_loaders[client_idx], client_optimizer, criterion, config.DEVICE)\n",
        "                    client_models.append(client_model)\n",
        "                # FedAvg aggregation\n",
        "                with torch.no_grad():\n",
        "                    for param in model.parameters():\n",
        "                        param.data.zero_()\n",
        "                    for client_model in client_models:\n",
        "                        for server_param, client_param in zip(model.parameters(), client_model.parameters()):\n",
        "                            server_param.data += client_param.data / len(selected_clients)\n",
        "                # Validation and checkpoint\n",
        "                if (round_num + 1) % 5 == 0 or round_num == NUM_ROUNDS - 1:\n",
        "                    val_loss, val_acc = evaluate(model, val_loader, criterion, config.DEVICE)\n",
        "                    val_losses.append(val_loss)\n",
        "                    val_accs.append(val_acc)\n",
        "                    if val_acc > best_val_acc:\n",
        "                        best_val_acc = val_acc\n",
        "                        best_model_state = copy.deepcopy(model.state_dict())\n",
        "                    print(f\"Round {round_num + 1}/{NUM_ROUNDS}: Val Acc {val_acc:.4f}\")\n",
        "                    torch.save({\n",
        "                        'round': round_num,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                        'best_model_state': best_model_state,\n",
        "                        'best_val_acc': best_val_acc,\n",
        "                        'val_losses': val_losses,\n",
        "                        'val_accs': val_accs,\n",
        "                    }, checkpoint_path)\n",
        "            if best_model_state is not None:\n",
        "                model.load_state_dict(best_model_state)\n",
        "            test_loss, test_acc = evaluate(model, test_loader, criterion, config.DEVICE)\n",
        "            print(f\"Best Val Acc: {best_val_acc:.3f}\")\n",
        "            print(f\"Test Acc: {test_acc:.3f}\")\n",
        "            plot_curves([], val_losses, [], val_accs, title=f\"nc={nc}, local_epochs={local_epochs}\")\n",
        "            results[(nc, local_epochs)] = {\n",
        "                \"test_acc\": test_acc,\n",
        "                \"best_val_acc\": best_val_acc,\n",
        "                \"val_accs\": val_accs,\n",
        "                \"val_losses\": val_losses\n",
        "            }\n",
        "    # Present results\n",
        "    df = present_fedavg_noniid_results(results)\n",
        "    return df\n",
        "\n",
        "def present_fedavg_noniid_results(results, save_csv=True, csv_path=\"fedavg_noniid_results.csv\"):\n",
        "    \"\"\"\n",
        "    Presents the results of run_fedavg_noniid_experiments in a table and heatmap, and optionally saves as CSV.\n",
        "    Args:\n",
        "        results: dict, keys: (nc, local_epochs), values: dict with 'test_acc' and 'best_val_acc'.\n",
        "        save_csv: whether to save as CSV.\n",
        "        csv_path: output CSV path.\n",
        "    Returns:\n",
        "        DataFrame of results.\n",
        "    \"\"\"\n",
        "    # Convert results dict to DataFrame\n",
        "    rows = []\n",
        "    for (nc, local_epochs), result in results.items():\n",
        "        rows.append({\n",
        "            \"Classes per Client (nc)\": nc,\n",
        "            \"Local Epochs\": local_epochs,\n",
        "            \"Best Val Acc\": result[\"best_val_acc\"],\n",
        "            \"Test Acc\": result[\"test_acc\"]\n",
        "        })\n",
        "    df = pd.DataFrame(rows)\n",
        "    df = df.sort_values([\"Classes per Client (nc)\", \"Local Epochs\"])\n",
        "    print(\"\\nFedAvg Non-IID Results Summary:\")\n",
        "    print(df.to_string(index=False))\n",
        "    if save_csv:\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"\\nResults saved to {csv_path}\")\n",
        "\n",
        "    # Plot Heatmap of Test Accuracies\n",
        "    pivot = df.pivot(index=\"Classes per Client (nc)\", columns=\"Local Epochs\", values=\"Test Acc\")\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.heatmap(pivot, annot=True, fmt=\".3f\", cmap=\"Blues\")\n",
        "    plt.title(\"FedAvg Non-IID: Test Accuracy Heatmap\")\n",
        "    plt.ylabel(\"Classes per Client (nc)\")\n",
        "    plt.xlabel(\"Local Epochs\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return df"
      ],
      "metadata": {
        "id": "BnqAmh7dTLEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_fedavg_noniid_experiments()"
      ],
      "metadata": {
        "id": "KVCGeN8aXlGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_sparse_experiments(plot_mode=\"best\", save_csv=True, csv_path=\"sparse_experiment_results.csv\"):\n",
        "    \"\"\"\n",
        "    Runs sparse training experiments, varying method and sparsity.\n",
        "    Presents results as a table, saves as CSV, and plots learning curves only for the best run (or other option).\n",
        "\n",
        "    Args:\n",
        "        plot_mode (str): \"best\" (default, plot only best run curves), \"all\" (plot all runs), \"none\" (no plots).\n",
        "        save_csv (bool): If True, saves results as csv.\n",
        "        csv_path (str): Path to save csv file.\n",
        "\n",
        "    Returns:\n",
        "        results (list): list of dicts, each with 'method', 'sparsity', 'accuracy'.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    config = Config()\n",
        "    os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
        "    checkpoint_dir = os.path.join(config.CHECKPOINT_DIR, \"sparse\")\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    methods = ['least-sensitive', 'most-sensitive', 'low-magnitude', 'high-magnitude', 'random', 'hybrid']\n",
        "    sparsity_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "    results = []\n",
        "\n",
        "    # --- Use full training/validation/test sets (not federated partitions) ---\n",
        "    transform_train, transform_test = get_transforms()\n",
        "    full_trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    trainset, valset = split_dataset(full_trainset, val_split=config.VAL_SPLIT)\n",
        "    train_loader = DataLoader(trainset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=config.PIN_MEMORY)\n",
        "    val_loader = DataLoader(valset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS, pin_memory=config.PIN_MEMORY)\n",
        "    test_loader = DataLoader(testset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS, pin_memory=config.PIN_MEMORY)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    best_result = None\n",
        "    best_acc = -float('inf')\n",
        "    best_curves = None\n",
        "\n",
        "    for method in methods:\n",
        "        method_dir = os.path.join(checkpoint_dir, method)\n",
        "        os.makedirs(method_dir, exist_ok=True)\n",
        "        for sparsity in sparsity_ratios:\n",
        "            print(f\"\\nMethod: {method}, Sparsity: {sparsity}\")\n",
        "            ckpt_path = os.path.join(method_dir, f\"sparsity_{sparsity}.pt\")\n",
        "            model = get_model(config.NUM_CLASSES, config.DEVICE)\n",
        "\n",
        "            # Use full training loader for mask calibration!\n",
        "            if method == 'hybrid':\n",
        "                mask = hybrid_mask(model, train_loader, config.DEVICE, sparsity=sparsity, alpha=0.5)\n",
        "            else:\n",
        "                mask = calibrate_mask(model, train_loader, config.DEVICE, method=method, sparsity=sparsity)\n",
        "\n",
        "            optimizer = SparseSGDM(model.parameters(), lr=config.LR, momentum=config.MOMENTUM,\n",
        "                                   weight_decay=config.WEIGHT_DECAY, mask=mask)\n",
        "            start_epoch = 0\n",
        "            if os.path.exists(ckpt_path):\n",
        "                checkpoint = torch.load(ckpt_path, map_location=config.DEVICE)\n",
        "                model.load_state_dict(checkpoint['model_state'])\n",
        "                optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "                start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "            train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "            for epoch in range(start_epoch, 5):\n",
        "                trl, tra = train(model, train_loader, optimizer, criterion, config.DEVICE)  # Use full train_loader\n",
        "                vall, vala = evaluate(model, val_loader, criterion, config.DEVICE)\n",
        "                train_losses.append(trl)\n",
        "                val_losses.append(vall)\n",
        "                train_accs.append(tra)\n",
        "                val_accs.append(vala)\n",
        "                print(f\"Epoch {epoch}: Train Acc {tra:.3f}, Val Acc {vala:.3f}\")\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state': model.state_dict(),\n",
        "                    'optimizer_state': optimizer.state_dict()\n",
        "                }, ckpt_path)\n",
        "\n",
        "            tloss, tacc = evaluate(model, test_loader, criterion, config.DEVICE)\n",
        "            print(f\"Test Accuracy for method {method}, sparsity {sparsity}: {tacc:.3f}\")\n",
        "            results.append({'method': method, 'sparsity': sparsity, 'accuracy': tacc})\n",
        "\n",
        "            # Track the best run and plot if needed\n",
        "            if tacc > best_acc:\n",
        "                best_acc = tacc\n",
        "                best_result = {'method': method, 'sparsity': sparsity, 'accuracy': tacc}\n",
        "                best_curves = (train_losses, val_losses, train_accs, val_accs)\n",
        "            if plot_mode == \"all\":\n",
        "                plot_curves(train_losses, val_losses, train_accs, val_accs)\n",
        "\n",
        "    # Present results as DataFrame\n",
        "    df = pd.DataFrame(results)\n",
        "    df_pivot = df.pivot(index=\"method\", columns=\"sparsity\", values=\"accuracy\")\n",
        "    print(\"\\nSparse Experiment Results (Test Accuracy):\")\n",
        "    print(df_pivot.round(3))\n",
        "\n",
        "    if save_csv:\n",
        "        df_pivot.to_csv(csv_path)\n",
        "        print(f\"\\nResults saved to {csv_path}\")\n",
        "\n",
        "    if plot_mode == \"best\" and best_curves is not None:\n",
        "        print(f\"\\nBest result: {best_result}\")\n",
        "        plot_curves(*best_curves)\n",
        "    return results"
      ],
      "metadata": {
        "id": "V5SqNtGwKhIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_sparse_experiments(plot_mode=\"best\", save_csv=True, csv_path=\"sparse_experiment_results.csv\")"
      ],
      "metadata": {
        "id": "6MbVX2NIXvYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_overlap_analysis(num_clients=5, seed=42, plot=True, save_csv=False, csv_path=\"mask_overlap_results.csv\"):\n",
        "    \"\"\"\n",
        "    Computes and presents the pairwise Jaccard overlap (intersection/union) between masks generated for different clients,\n",
        "    across various mask calibration methods and sparsity levels.\n",
        "\n",
        "    Args:\n",
        "        num_clients (int): Number of clients to consider.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "        plot (bool): If True, displays a heatmap of the results.\n",
        "        save_csv (bool): If True, saves the results as a CSV file.\n",
        "        csv_path (str): Path to save the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        overlaps (dict): mapping (method, sparsity) -> average pairwise Jaccard index (float)\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    config = Config()\n",
        "    clients_loaders, _, _ = get_dataloaders(config, iid=False, nc=config.NC)\n",
        "    method_list = ['least-sensitive', 'most-sensitive', 'low-magnitude', 'high-magnitude', 'random']\n",
        "    ratio_list = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "    overlaps = {}\n",
        "    print(\"Computing mask overlaps...\")\n",
        "\n",
        "    for method in method_list:\n",
        "        for sparsity in ratio_list:\n",
        "            masks = []\n",
        "            for loader in clients_loaders[:num_clients]:\n",
        "                # Always start from a fresh model for each mask (to avoid contamination)\n",
        "                model = get_model(config.NUM_CLASSES, config.DEVICE)\n",
        "                mask = calibrate_mask(model, loader, config.DEVICE, method=method, sparsity=sparsity, seed=seed)\n",
        "                masks.append(mask)\n",
        "            # Compute all pairwise Jaccard indices\n",
        "            jaccs = []\n",
        "            for i in range(len(masks)):\n",
        "                for j in range(i + 1, len(masks)):\n",
        "                    mask1 = torch.cat([m.flatten() for m in masks[i]])\n",
        "                    mask2 = torch.cat([m.flatten() for m in masks[j]])\n",
        "                    intersection = (mask1 * mask2).sum().item()\n",
        "                    union = ((mask1 + mask2) > 0).sum().item()\n",
        "                    jaccs.append(intersection / union if union > 0 else 0)\n",
        "            overlaps[(method, sparsity)] = float(np.mean(jaccs)) if jaccs else 0.0\n",
        "            print(f\"Method: {method:>15}, Sparsity: {sparsity:.1f}, Avg Jaccard: {overlaps[(method, sparsity)]:.3f}\")\n",
        "\n",
        "    # Convert to DataFrame for presentation\n",
        "    records = []\n",
        "    for (method, sparsity), jacc in overlaps.items():\n",
        "        records.append({\"Method\": method, \"Sparsity\": sparsity, \"Avg_Jaccard\": jacc})\n",
        "    df = pd.DataFrame(records)\n",
        "    df_pivot = df.pivot(index=\"Method\", columns=\"Sparsity\", values=\"Avg_Jaccard\")\n",
        "    print(\"\\nMask Overlap (Average Jaccard Index):\")\n",
        "    print(df_pivot.round(3))\n",
        "\n",
        "    if save_csv:\n",
        "        df_pivot.to_csv(csv_path)\n",
        "        print(f\"\\nResults saved to {csv_path}\")\n",
        "\n",
        "    if plot:\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        sns.heatmap(df_pivot, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
        "        plt.title(\"Average Jaccard Index between Client Masks\")\n",
        "        plt.ylabel(\"Method\")\n",
        "        plt.xlabel(\"Sparsity\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return overlaps"
      ],
      "metadata": {
        "id": "Yz5_sGy0KHKj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}